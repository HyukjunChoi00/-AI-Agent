import requests
from bs4 import BeautifulSoup
from smolagents import Tool
import urllib.parse
from typing import List, Dict
import re
import time
import random
from smolagents import CodeAgent
from smolagents.models import TransformersModel

class FinancialTimesSearchTool(Tool):
    """
    Financial Times 웹사이트에서 검색을 수행하는 도구
    Google 검색을 통해 FT 사이트 내 검색을 수행합니다.
    """
    
    name = "financial_times_search"
    description = """
    Financial Times 웹사이트에서 주어진 키워드나 문구로 검색을 수행합니다.
    Google 검색을 통해 FT 사이트 내 최신 금융, 경제, 비즈니스 뉴스와 분석 기사를 찾습니다.
    검색 결과로 기사 제목, 날짜, 요약, 링크 정보를 제공합니다.
    """
    
    inputs = {
        "query": {
            "type": "string", 
            "description": "검색할 키워드나 문구 (예: 'nvidia', 'artificial intelligence', 'stock market')"
        },
        "include_content": {
            "type": "boolean",
            "description": "기사 내용을 크롤링하여 요약할지 여부 (기본값: False)",
            "default": False,
            "nullable": True
        }
    }
    
    output_type = "string"
    
    def _get_session(self):
        """요청 세션을 설정합니다."""
        session = requests.Session()
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0'
        }
        
        session.headers.update(headers)
        return session
    
    
    
    def _extract_real_url(self, duckduckgo_url: str) -> str:
        """DuckDuckGo 리다이렉트 URL에서 실제 URL을 추출합니다."""
        try:
            if 'uddg=' in duckduckgo_url:
                # URL 디코딩
                import urllib.parse
                decoded = urllib.parse.unquote(duckduckgo_url.split('uddg=')[1].split('&')[0])
                return decoded
            return duckduckgo_url
        except Exception:
            return duckduckgo_url
    
    def _extract_article_content(self, url: str) -> str:
        """기사 URL에서 내용을 추출합니다."""
        try:
            session = self._get_session()
            
            # DuckDuckGo 리다이렉트 URL 처리
            real_url = self._extract_real_url(url)
            
            # 기사 페이지 요청
            response = session.get(real_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # FT 기사 구조에 맞는 선택자들
            content_selectors = [
                '.article-body',
                '.n-content-body',
                '.article-content',
                '[data-module-name="article-body"]',
                '.story-body',
                '.content-body'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # 불필요한 요소 제거
                    for unwanted in content_elem.find_all(['script', 'style', 'nav', 'aside', 'footer']):
                        unwanted.decompose()
                    
                    # 텍스트 추출
                    paragraphs = content_elem.find_all(['p', 'div'])
                    text_parts = []
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if text and len(text) > 20:  # 의미있는 텍스트만
                            text_parts.append(text)
                    
                    content = ' '.join(text_parts)
                    break
            
            # 만약 특정 선택자로 찾지 못했다면 일반적인 방법 시도
            if not content:
                # 모든 p 태그에서 텍스트 추출
                paragraphs = soup.find_all('p')
                text_parts = []
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if text and len(text) > 30:
                        text_parts.append(text)
                
                content = ' '.join(text_parts[:10])  # 최대 10개 문단
            
            return content[:2000] if content else ""  # 최대 2000자
            
        except Exception as e:
            return f"기사 내용 추출 실패: {str(e)}"
    
    def _summarize_content(self, content: str, max_length: int = 300) -> str:
        """LLM agent를 사용하여 기사 내용을 요약합니다."""
        if not content or len(content) < 100:
            return content
        
        try:
            # LLM agent 초기화
            model = TransformersModel(model_id="naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B")
            agent = CodeAgent(tools=[], model=model)
            
            # 요약 프롬프트 구성
            prompt = f"""
다음은 Financial Times의 기사 내용입니다. 이 기사를 한국어로 간결하게 요약해주세요.

요약 요구사항:
1. 핵심 내용과 주요 사실을 포함
2. 금융/경제 관련 수치나 데이터가 있다면 반드시 포함
3. {max_length}자 이내로 작성
4. 객관적이고 명확한 문체 사용

기사 내용:
{content[:3000]}  # 너무 긴 경우 앞부분만 사용

요약:
"""
            
            # LLM agent에게 요약 요청
            response = agent.run(prompt)
            
            # 응답에서 요약 부분만 추출
            if response and len(response) > 0:
                # 응답이 너무 길면 자르기
                if len(response) > max_length:
                    response = response[:max_length] + "..."
                return response
            else:
                # LLM 요약 실패 시 기존 방식으로 폴백
                return self._fallback_summarize(content, max_length)
                
        except Exception as e:
            print(f"LLM 요약 실패: {str(e)}")
            # LLM 요약 실패 시 기존 방식으로 폴백
            return self._fallback_summarize(content, max_length)
            
    
    def _search_via_duckduckgo(self, query: str) -> List[Dict]:
        """DuckDuckGo를 통해 FT 사이트 내 검색을 수행합니다."""
        try:
            session = self._get_session()
            
            # DuckDuckGo 검색 쿼리
            search_query = f"site:ft.com {query}"
            encoded_query = urllib.parse.quote(search_query)
            ddg_url = f"https://duckduckgo.com/html/?q={encoded_query}"
            
            time.sleep(random.uniform(1, 2))
            
            response = session.get(ddg_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            results = []
            
            # DuckDuckGo 검색 결과 파싱
            search_results = soup.find_all('div', class_='result')
            
            for result in search_results[:8]:
                try:
                    # 제목과 링크 추출
                    title_elem = result.find('a', class_='result__a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    link = title_elem.get('href', '')
                    
                    # FT 링크인지 확인
                    if 'ft.com' not in link:
                        continue
                    
                    # 요약 추출
                    summary = ""
                    summary_elem = result.find('a', class_='result__snippet')
                    if summary_elem:
                        summary = summary_elem.get_text(strip=True)
                    
                    if title and link:
                        results.append({
                            'title': title,
                            'link': link,
                            'summary': summary,
                            'date': ''
                        })
                        
                except Exception:
                    continue
            
            return results
            
        except Exception:
            return []
    
    def _direct_ft_search(self, query: str) -> List[Dict]:
        """FT 사이트에서 직접 검색을 시도합니다."""
        try:
            session = self._get_session()
            
            # FT RSS 피드나 다른 공개 API 시도
            # 예: FT의 RSS 피드 활용
            rss_urls = [
                "https://www.ft.com/rss/home/uk",
                "https://www.ft.com/rss/home/us", 
                "https://www.ft.com/rss/companies/technology"
            ]
            
            results = []
            for rss_url in rss_urls:
                try:
                    response = session.get(rss_url, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'xml')
                        items = soup.find_all('item')
                        
                        for item in items:
                            title_elem = item.find('title')
                            link_elem = item.find('link')
                            desc_elem = item.find('description')
                            date_elem = item.find('pubDate')
                            
                            if title_elem and link_elem:
                                title = title_elem.get_text(strip=True)
                                link = link_elem.get_text(strip=True)
                                
                                # 검색어와 관련성 확인
                                if query.lower() in title.lower() or (desc_elem and query.lower() in desc_elem.get_text().lower()):
                                    results.append({
                                        'title': title,
                                        'link': link,
                                        'summary': desc_elem.get_text(strip=True) if desc_elem else '',
                                        'date': date_elem.get_text(strip=True) if date_elem else ''
                                    })
                                    
                                    if len(results) >= 5:
                                        break
                except Exception:
                    continue
                    
                if results:
                    break
            
            return results
            
        except Exception:
            return []
    
    def forward(self, query: str, include_content: bool = False) -> str:
        """
        Financial Times에서 검색을 수행하고 결과를 반환합니다.
        
        Args:
            query (str): 검색할 키워드나 문구
            include_content (bool): 기사 내용을 크롤링하여 요약할지 여부
            
        Returns:
            str: 검색 결과를 포함한 텍스트 (제목, 요약, 링크 등)
        """
        try:
            results = []
            # DuckDuckGo 시도
            results = self._search_via_duckduckgo(query)
            
            # 결과 포맷팅
            if results:
                final_result = f"'{query}' 검색 결과 (Financial Times):\n\n"
                for i, result in enumerate(results, 1):
                    final_result += f"{i}. 제목: {result['title']}\n"
                    if result.get('date'):
                        final_result += f"   날짜: {result['date']}\n"
                    if result.get('summary'):
                        summary = result['summary']
                        if len(summary) > 200:
                            summary = summary[:200] + "..."
                        final_result += f"   요약: {summary}\n"
                    
                    # include_content가 True인 경우 기사 내용 크롤링
                    if include_content and result.get('link'):
                        print(f"기사 내용 크롤링 중... ({i}/{len(results)})")
                        content = self._extract_article_content(result['link'])
                        if content and not content.startswith("기사 내용 추출 실패"):
                            summarized_content = self._summarize_content(content)
                            final_result += f"   기사 내용 요약: {summarized_content}\n"
                    
                    if result.get('link'):
                        final_result += f"   링크: {result['link']}\n"
                    final_result += "\n"
                
                return final_result
            else:
                # 수동 검색 링크 제공
                manual_link = f"https://www.google.com/search?q=site:ft.com+{urllib.parse.quote(query)}"
                return f"'{query}'에 대한 검색 결과를 자동으로 찾을 수 없습니다.\n\n다음 링크에서 수동으로 검색해보세요:\n{manual_link}\n\n또는 직접 FT 사이트에서 검색:\nhttps://www.ft.com/search?q={urllib.parse.quote(query)}"
                
        except Exception as e:
            return f"검색 중 오류가 발생했습니다: {str(e)}"

# 사용 예시 및 테스트
if __name__ == "__main__":
    # 도구 인스턴스 생성
    ft_search = FinancialTimesSearchTool()
    
    # 테스트
    print("=== NVIDIA 검색 테스트 ===")
    result = ft_search.forward("nvidia", include_content=False)
    print(result)
    
    print("\n=== AI 검색 테스트 (내용 포함) ===")
    result2 = ft_search.forward("artificial intelligence", include_content=True)
    print(result2)
