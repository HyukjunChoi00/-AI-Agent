### Powershellì—ì„œ streamlit run ctest.py   ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.

import json
from typing import TypedDict, Annotated, List, Dict, Optional
import operator
import os
import asyncio
import re
from urllib.parse import urlparse, urljoin
import nest_asyncio
nest_asyncio.apply()
import sys
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
import streamlit as st
from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
)
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser

# ì „ë¬¸ìš©ì–´ ì£¼ì„ ê¸°ëŠ¥ì„ ìœ„í•œ ì¶”ê°€ import
from datasets import load_dataset
from langchain_community.document_loaders import PyPDFLoader
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.retrievers import BM25Retriever

# Playwright ê´€ë ¨ ì„í¬íŠ¸
try:
    from playwright.async_api import async_playwright
    from bs4 import BeautifulSoup, Comment
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    st.error("Playwright ë˜ëŠ” BeautifulSoupì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
    st.code("pip install playwright beautifulsoup4")
    st.code("playwright install chromium")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["CLOVASTUDIO_API_KEY"] = ''

from langchain_naver import ChatClovaX
  
chat = ChatClovaX(
    model="HCX-007" # ëª¨ë¸ëª… ì…ë ¥ (ê¸°ë³¸ê°’: HCX-005) 
)

###########################################
# RAG ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
pdf_docs = []
pdf_loader = PyPDFLoader("ê²½ì œê¸ˆìœµìš©ì–´ 700ì„ .pdf")
pdf_docs = pdf_loader.load()
qa_dataset = load_dataset("coorung/Kor-financial-qa-7K", split="train")
qa_docs = []
for example in qa_dataset:
    if "query" in example and "pos" in example and example["pos"]:
    text = f"ì§ˆë¬¸: {example['query']}\në‹µë³€: {example['pos'][0]}"
    qa_docs.append(Document(page_content=text))
all_docs = pdf_docs + qa_docs
splitter = RecursiveCharacterTextSplitter(chunk_size=642, chunk_overlap=50)
split_docs = splitter.split_documents(all_docs)
retriever = BM25Retriever.from_documents(split_docs)
self.qa_chain = RetrievalQA.from_chain_type(
    llm=chat,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
)
###########################################

# ì „ë¬¸ìš©ì–´ ì§ˆì˜ ê·¸ë˜í”„





##############################################

 

### Econotimesë¡œë¶€í„° ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ í´ë˜ìŠ¤ ì •ì˜  ##################

class EconoTimesFullPipeline:
    def __init__(self, max_articles=5):
        self.max_articles = max_articles
        self.base_url = "https://www.econotimes.com"

    async def setup_browser(self):
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(headless=True)
        self.context = await self.browser.new_context()

    async def close_browser(self):
        await self.browser.close()
        await self.playwright.stop()

    def clean_title(self, title: str) -> str:
        title = re.sub(r'\s+', ' ', title.strip())
        title = re.sub(r'^\W+|\W+$', '', title)
        return title if len(title) > 10 else ""

    async def extract_articles(self, page, query: str) -> List[Dict[str, str]]:
        articles = []
        for i in range(1, self.max_articles + 1):
            try:
                xpath = f'//*[@id="archivePage"]/div/div[2]/div[{i}]/p[1]/a'
                link = await page.query_selector(f'xpath={xpath}')
                if not link:
                    continue
                title = await link.inner_text()
                url = await link.get_attribute('href')
                url = urljoin(self.base_url, url) if url and not url.startswith("http") else url
                title = self.clean_title(title)
                if title and url:
                    articles.append({"title": title, "url": url})
            except:
                continue
        return articles

    async def fetch_html(self, url: str) -> str:
        page = await self.context.new_page()
        await page.goto(url, wait_until='domcontentloaded', timeout=20000)
        await asyncio.sleep(3)
        html = await page.inner_html("body")
        await page.close()
        return html

    def clean_html(self, raw_html: str) -> str:
        soup = BeautifulSoup(raw_html, 'html.parser')
        for tag in soup.find_all(True):
            tag.attrs = {}
        for tag in soup(['script', 'style', 'header', 'footer', 'nav']):
            tag.decompose()
        for comment in soup.find_all(text=lambda t: isinstance(t, Comment)):
            comment.extract()
        return str(soup)

    def extract_text(self, html: str) -> str:
        soup = BeautifulSoup(html, 'html.parser')
        article = soup.find('article')
        if not article:
            return "ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        for tag in article(['ins', 'iframe', 'script']):
            tag.decompose()
        paragraphs = [p.get_text(strip=True) for p in article.find_all('p')]
        return '\n\n'.join(paragraphs)

    async def run_pipeline(self, query: str) -> List[Dict[str, str]]:
        await self.setup_browser()
        search_url = f"https://www.econotimes.com/search?v={query}"
        page = await self.context.new_page()
        await page.goto(search_url, wait_until='domcontentloaded', timeout=20000)
        await asyncio.sleep(2)
        articles = await self.extract_articles(page, query)
        await page.close()

        results = []
        for article in articles:
            html = await self.fetch_html(article["url"])
            cleaned_html = self.clean_html(html)
            text = self.extract_text(cleaned_html)
            results.append({
                "title": article["title"],
                "url": article["url"],
                "content": text
            })

        await self.close_browser()
        return results

####################################################################################################################
## ì¿¼ë¦¬ í™•ì¥ íŒŒíŠ¸

class ExpandedQuery(BaseModel):
    expanded_search_query_list: List[str] = Field(description="ì§ˆì˜ í™•ì¥ëœ ê²€ìƒ‰ í‚¤ì›Œë“œí˜• ì§ˆì˜ ë¦¬ìŠ¤íŠ¸. í‚¤ì›Œë“œí˜• ê²€ìƒ‰ ì—”ì§„ì„ ìœ„í•´")

# í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ Chain - JSON íŒŒì‹± ë°©ì‹
def build_keyword_extraction_chain():
    prompt_template = ChatPromptTemplate.from_messages(
        [("system", "ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ í•µì‹¬ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” Assistantì´ë‹¤."),
         ("user", "ë‹¤ìŒ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ë¼. ë°˜ë“œì‹œ ì˜ë¬¸ í‚¤ì›Œë“œë¡œ ì œê³µí•˜ë¼.\n"
                  "ì˜ˆì‹œ:\n"
                  "- 'NVIDIA ì£¼ì‹ ë™í–¥ ì¡°ì‚¬ì¢€ í•´ì¤˜' â†’ 'NVIDIA'\n"
                  "- 'í…ŒìŠ¬ë¼ ì£¼ê°€ ì „ë§ì´ ì–´ë•Œ?' â†’ 'Tesla'\n"
                  "- 'ì• í”Œ ì•„ì´í° ìµœì‹  ì†Œì‹ ì•Œë ¤ì¤˜' â†’ 'Apple'\n"
                  "- 'AI ê¸°ìˆ  ë°œì „ í˜„í™©' â†’ 'AI'\n"
                  "ì§ˆë¬¸: {query}\n\n"
                  "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì¤˜:\n"
                  '{{"expanded_search_query_list": ["ì¶”ì¶œëœ_í‚¤ì›Œë“œ"]}}\n'
                  "í•µì‹¬ í‚¤ì›Œë“œ:")]
    )
    
    return prompt_template | chat

# Chain ì •ì˜ - ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
def build_query_expansion_chain():
    prompt_template = ChatPromptTemplate.from_messages(
        [("system", "ë„ˆëŠ” í‚¤ì›Œë“œí˜• Query Expansion Assistantì´ë‹¤. ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ì‚°ì—…, ê¸°ìˆ , ì‹œì¥ í‚¤ì›Œë“œë¡œ í™•ì¥í•´ì•¼ í•œë‹¤."),
         ("user", "ë‹¤ìŒ í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ë¼:\n"
                  "ì›ë³¸ í‚¤ì›Œë“œ: {query}\n"
                  "í™•ì¥í•  ê°œìˆ˜: {n}ê°œ\n\n"
                  "í™•ì¥ ê·œì¹™:\n"
                  "- Tesla â†’ electric vehicle, EV, battery\n"
                  "- NVIDIA â†’ AI, GPU, semiconductor\n"
                  "- Apple â†’ iPhone, technology, consumer electronics\n"
                  "- Microsoft â†’ cloud computing, software, Azure\n\n"
                  "'{query}' í‚¤ì›Œë“œì— ëŒ€í•´ ê´€ë ¨ ì‚°ì—…/ê¸°ìˆ  í‚¤ì›Œë“œ {n}ê°œë¥¼ ì˜ë¬¸ìœ¼ë¡œ ìƒì„±í•˜ë¼.\n\n"
                  "ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë¼ (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ í¬í•¨ ê¸ˆì§€):\n"
                  '{{"expanded_search_query_list": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"]}}\n')]
    )
    
    return prompt_template | chat

#######################################################################################################################

class NewsAnalysis(BaseModel):
    """ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼"""
    has_relevant_news: bool = Field(description="ê´€ë ¨ ë‰´ìŠ¤ê°€ ìˆëŠ”ì§€ ì—¬ë¶€")
    analysis_summary: str = Field(description="ë‰´ìŠ¤ ë¶„ì„ ìš”ì•½. bullet pointë¥¼ í™œìš©í•œ ê°œì¡°ì‹ ë‹µë³€. ë§Œì•½ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ê³µë°±ìœ¼ë¡œ ë°˜í™˜")

def build_news_analysis_chain():
    prompt_template = ChatPromptTemplate.from_messages(
        [("system", "ë„ˆëŠ” ê²½ì œ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì´ë‹¤."),
         ("user", "ë‹¤ìŒ ìœ ì €ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë‰´ìŠ¤ë‚´ì— ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¶„ì„ì„ ë§Œë“¤ì–´ë¼. ë˜í•œ, ê¸ˆë¦¬ì™€ ê´€ì„¸ì™€ ê°™ì€ ì¼ë°˜ì ì¸ ê²½ì œ ìƒí™©ì„ ë‹¤ë£¨ëŠ” ì •ë³´ë„ ì „ë‹¬í•˜ë¼. ê¸ˆë¦¬ì˜ ê²½ìš° ëª…í™•í•œ ìˆ˜ì¹˜ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í‘œê¸°í•˜ë¼.\n"
                  "ì§ˆë¬¸: {query}\n"
                  "ê²€ìƒ‰ëœ ë‰´ìŠ¤: {news}\n"
                  "ê²€ìƒ‰ëœ ë‰´ìŠ¤ì—ì„œ '{query}'ì— ëŒ€í•´ ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¶„ì„ì„ ë§Œë“¤ì–´ë¼\n"
                  "ì‚¬ìš©ìê°€ íŠ¹ì • ì£¼ì‹ ì¢…ëª©ì— ëŒ€í•œ ë¶„ì„ì„ ìš”ì²­í–ˆë‹¤ë©´, ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”:\n"
                  "1. ì£¼ìš” íŠ¸ë Œë“œì™€ ë™í–¥ ìš”ì•½\n"
                  "2. ì‹œì¥ ì˜í–¥ ë¶„ì„\n"
                  "3. í–¥í›„ ì „ë§ì´ë‚˜ ì£¼ëª©í•  ì \n"
                  "4. ì „ë°˜ì ì¸ ê²½ì œ ìƒí™©\n\n"
                  "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì¤˜:\n"
                  '{{"has_relevant_news": true/false, "analysis_summary": "ë¶„ì„ë‚´ìš©_ë˜ëŠ”_ë¹ˆë¬¸ìì—´"}}\n')]
    )
    
    return prompt_template | chat

# ê·¸ë˜í”„ ìƒíƒœ ì •ì˜
class State(TypedDict):
    query: str
    extracted_keyword: str
    expanded_query_list: List[str]
    analysis_list: List[str]
    answer: str
    messages: Annotated[List[BaseMessage], operator.add]
    search_results: List[dict]
    search_results2 : List[dict]
    annotated_answer : str

# JSON íŒŒì‹± í—¬í¼ í•¨ìˆ˜ - ë” ì•ˆì „í•œ ë²„ì „
def parse_json_response(response) -> dict:
    """JSON ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        import json
        
        # AIMessage ê°ì²´ì—ì„œ content ì¶”ì¶œ
        if hasattr(response, 'content'):
            content = str(response.content)
        else:
            content = str(response)
        
        print(f"[DEBUG] Raw response content: {content[:200]}...")  # ì²˜ìŒ 200ìë§Œ ì¶œë ¥
        
        # JSON ë¸”ë¡ ì°¾ê¸°
        if '```json' in content:
            start = content.find('```json') + 7
            end = content.find('```', start)
            json_str = content[start:end].strip()
        elif '{' in content and '}' in content:
            start = content.find('{')
            end = content.rfind('}') + 1
            json_str = content[start:end]
        else:
            print("[DEBUG] No JSON found in response")
            return {"expanded_search_query_list": []}
        
        print(f"[DEBUG] Extracted JSON: {json_str}")
        
        result = json.loads(json_str)
        print(f"[DEBUG] Parsed result: {result}")
        return result
        
    except json.JSONDecodeError as e:
        print(f"[DEBUG] JSON decode error: {e}")
        return {"expanded_search_query_list": []}
    except Exception as e:
        print(f"[DEBUG] General parsing error: {e}")
        return {"expanded_search_query_list": []}

# ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ - JSON íŒŒì‹± ë°©ì‹
def extract_keyword(state):
    """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë…¸ë“œ"""
    keyword_extraction_chain = build_keyword_extraction_chain()
    original_query = state["query"]
    
    try:
        print(f"[DEBUG] Starting keyword extraction for: {original_query}")
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        response = keyword_extraction_chain.invoke({"query": original_query})
        print(f"[DEBUG] Keyword extraction response type: {type(response)}")
        
        parsed_response = parse_json_response(response)
        
        if parsed_response.get('expanded_search_query_list'):
            keyword = parsed_response['expanded_search_query_list'][0].strip()
            print(f"[DEBUG] Extracted keyword: {keyword}")
        else:
            # ë°±ì—…: ì›ë³¸ ì¿¼ë¦¬ì—ì„œ ì˜ë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ
            keyword = original_query.strip()
            print(f"[DEBUG] Using original query as keyword: {keyword}")
            
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
        print(f"[DEBUG] Error in keyword extraction: {e}")
        st.warning(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        keyword = original_query.strip()
    
    return {"extracted_keyword": keyword}

def query_expansion(state):
    query_expansion_chain = build_query_expansion_chain()
    extracted_keyword = state["extracted_keyword"]
    
    try:
        print(f"[DEBUG] Starting query expansion for: {extracted_keyword}")
        response = query_expansion_chain.invoke({"query": extracted_keyword, "n": 2})
        print(f"[DEBUG] Query expansion response type: {type(response)}")
        
        parsed_response = parse_json_response(response)
        
        if parsed_response.get('expanded_search_query_list'):
            # ì›ë³¸ í‚¤ì›Œë“œ + í™•ì¥ëœ í‚¤ì›Œë“œë“¤
            expanded_list = parsed_response['expanded_search_query_list']
            expanded_query_list = [extracted_keyword] + expanded_list
            print(f"[DEBUG] Final expanded list: {expanded_query_list}")
        else:
            print("[DEBUG] No expanded queries found, using original keyword only")
            expanded_query_list = [extracted_keyword]
            
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
        print(f"[DEBUG] Error in query expansion: {e}")
        st.warning(f"ì¿¼ë¦¬ í™•ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        expanded_query_list = [extracted_keyword]

    return {"expanded_query_list": expanded_query_list}

############################################################################### ì•„ë˜ ë¹„ë™ê¸° í•¨ìˆ˜ ì²˜ë¦¬ê°€ ì œì¼ ë¬¸ì œì„.

async def search(state):
    expanded_query_list = state["expanded_query_list"]
    pipeline = EconoTimesFullPipeline(max_articles=3)
    all_results = []
    await pipeline.setup_browser()
    try:
        for query in expanded_query_list:
            try:
                results = await pipeline.run_pipeline(query)
                if not results:
                    # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë‹¤ìŒ ì¿¼ë¦¬ë¡œ
                    continue
                
                # ê° ê²°ê³¼ì— ê²€ìƒ‰ ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
                all_results.extend([{
                    "title": article["title"],
                    "url": article["url"],
                    "content": article["content"],
                    "search_query": query
                } for article in results])
                
            except Exception as e:
                # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê¹…í•˜ê±°ë‚˜ ë¬´ì‹œí•˜ê³  ë‹¤ìŒ ì¿¼ë¦¬ ê³„ì† ì§„í–‰ ê°€ëŠ¥
                # í•„ìš”í•˜ë©´ ì•„ë˜ ì£¼ì„ ì œê±° í›„ ë¡œê¹… ì¶”ê°€
                # print(f"Error processing query '{query}': {e}")
                continue
    finally:
        await pipeline.close_browser()

    
    return {"search_results": all_results}

#############################################################################

async def general_economics(state):
    pipeline = EconoTimesFullPipeline(max_articles=3)
    expanded_query_list = ['tariff', 'interest rate']
    all_results = []
    await pipeline.setup_browser()
    try:
        for query in expanded_query_list:
            try:
                results = await pipeline.run_pipeline(query)
                if not results:
                    # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë‹¤ìŒ ì¿¼ë¦¬ë¡œ
                    continue
                
                # ê° ê²°ê³¼ì— ê²€ìƒ‰ ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
                all_results.extend([{
                    "title": article["title"],
                    "url": article["url"],
                    "content": article["content"],
                    "search_query": query
                } for article in results])
                
            except Exception as e:
                # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê¹…í•˜ê±°ë‚˜ ë¬´ì‹œí•˜ê³  ë‹¤ìŒ ì¿¼ë¦¬ ê³„ì† ì§„í–‰ ê°€ëŠ¥
                # í•„ìš”í•˜ë©´ ì•„ë˜ ì£¼ì„ ì œê±° í›„ ë¡œê¹… ì¶”ê°€
                # print(f"Error processing query '{query}': {e}")
                continue
    finally:
        await pipeline.close_browser()

    
    return {"search_results2": all_results}

#############################################################################

def make_analysis_for_each_query(state):
    news_analysis_chain = build_news_analysis_chain()

    search_results = state["search_results"]
    search_results += state["search_results2"]
    
    analysis_list = []
    
    for doc in search_results:
        try:
            response = news_analysis_chain.invoke({"query": doc['search_query'], "news": doc['content']})
            parsed_response = parse_json_response(response)
            
            analysis_list.append({
                "has_relevant_news": parsed_response.get('has_relevant_news', False),
                "analysis_summary": parsed_response.get('analysis_summary', ''),
                "search_query": doc['search_query'],
                "title": doc['title'],
                "url": doc['url'],
                "content": doc['content']
            })
        except Exception as e:
            # ê°œë³„ ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬
            st.warning(f"ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            analysis_list.append({
                "has_relevant_news": False,
                "analysis_summary": "",
                "search_query": doc['search_query'],
                "title": doc['title'],
                "url": doc['url'],
                "content": doc['content']
            })
    
    return {"analysis_list": analysis_list}

#############################################################################

def generate_response(state):
    context = "\n\n".join([analysis['analysis_summary']
                           for analysis in state["analysis_list"]
                           if analysis['has_relevant_news']])
    
    curr_human_turn = HumanMessage(content=f"ì§ˆë¬¸: {state['query']}\n"
                            f"```\n{context}```"
                             "\n---\n"
                             "ì‘ë‹µì€ markdownì„ ì´ìš©í•´ ë¦¬í¬íŠ¸ ìŠ¤íƒ€ì¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ë¼. "
                             "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì˜ ì˜ë„ì— ë§ëŠ” ì •ë‹µ ë¶€ë¶„ì„ ê°•ì¡°í•´ë¼.")
    messages = state["messages"] + [curr_human_turn]
    response = llm.invoke(messages)
    
    # ì „ë¬¸ìš©ì–´ ì£¼ì„ ì¶”ê°€
    annotated_answer = term_annotator.explain_terms(response.content)

    return {"messages": [*messages, response],
            "answer": response.content,
            "annotated_answer" : annotated_answer}

################################################################ê·¸ë˜í”„ ì—°ê²° ë° streamlit ì ìš©

# Streamlit ì•± ì´ˆê¸°í™”
if 'graph' not in st.session_state:
    llm = chat

    # ê·¸ë˜í”„ êµ¬ì„±
    workflow = StateGraph(State)
    workflow.add_node("extract_keyword", extract_keyword)
    workflow.add_node("query_expansion", query_expansion)
    workflow.add_node("search", search)
    workflow.add_node("general economic", general_economics)
    workflow.add_node("make_analysis_for_each_query", make_analysis_for_each_query)
    workflow.add_node("generate", generate_response)
    workflow.add_edge("extract_keyword", "query_expansion")
    workflow.add_edge("query_expansion", "search")
    workflow.add_edge("search", "general economic")
    workflow.add_edge("general economic", "make_analysis_for_each_query")
    workflow.add_edge("make_analysis_for_each_query", "generate")

    workflow.set_entry_point("extract_keyword")
    workflow.set_finish_point("generate")

    graph = workflow.compile()

    st.session_state.graph = graph
    st.session_state.llm = llm

# ì „ë¬¸ìš©ì–´ ì£¼ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
if 'term_annotator_initialized' not in st.session_state:
    st.session_state.term_annotator_initialized = False

graph = st.session_state.graph 
llm = st.session_state.llm 

###############################################################################
# Streamlit View

st.title("ğŸ“° ê¸ˆìœµ íˆ¬ì ì§€ì› ì±—ë´‡")

# ì „ë¬¸ìš©ì–´ ì£¼ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì„¹ì…˜
st.markdown("### ğŸ”§ ì „ë¬¸ìš©ì–´ ì£¼ì„ ì‹œìŠ¤í…œ")
col1, col2 = st.columns([3, 1])

with col1:
    if not st.session_state.term_annotator_initialized:
        st.warning("âš ï¸ ì „ë¬¸ìš©ì–´ ì£¼ì„ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.info("ğŸ“Œ ì „ë¬¸ìš©ì–´ ì£¼ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ ì´ˆê¸°í™” ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
    else:
        st.success("âœ… ì „ë¬¸ìš©ì–´ ì£¼ì„ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

with col2:
    if st.button("ğŸ”§ ì „ë¬¸ìš©ì–´ ì£¼ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”", type="primary"):
        with st.spinner("ì „ë¬¸ìš©ì–´ ì£¼ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
            term_annotator.initialize()
            if term_annotator.is_initialized:
                st.session_state.term_annotator_initialized = True
                st.rerun()

st.markdown("---")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ğŸ¤– ì±—ë´‡ ì„ íƒ")
    
    chatbot_mode = st.selectbox(
        "ì±—ë´‡ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        ["íˆ¬ì ì˜ì‚¬ê²°ì • ì§€ì› ì±—ë´‡", "ê¸ˆìœµ ì‚¬ì „ ì±—ë´‡"],
        index=0
    )
    
    st.markdown("---")
    
    if chatbot_mode == "íˆ¬ì ì˜ì‚¬ê²°ì • ì§€ì› ì±—ë´‡":
        st.markdown("### ğŸ“ˆ íˆ¬ì ì˜ì‚¬ê²°ì • ì§€ì›")
        st.markdown("**ê¸°ëŠ¥:**")
        st.markdown("- ì‹¤ì‹œê°„ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„")
        st.markdown("- ì¢…ëª©ë³„ ì‹œì¥ ë™í–¥ ë¶„ì„")
        st.markdown("- íˆ¬ì ê´€ë ¨ ë¦¬í¬íŠ¸ ìƒì„±")
        st.markdown("- ì „ë¬¸ìš©ì–´ ìë™ ì£¼ì„")
        
        st.markdown("### ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ")
        st.markdown("- NVIDIA ì£¼ì‹ ë™í–¥")
        st.markdown("- Tesla ì „ë§")
        st.markdown("- Apple ì‹¤ì  ë¶„ì„")
        st.markdown("- AI ê´€ë ¨ íˆ¬ì")
        
    else:
        st.markdown("### ğŸ“š ê¸ˆìœµ ì‚¬ì „")
        st.markdown("**ê¸°ëŠ¥:**")
        st.markdown("- ê¸ˆìœµ ì „ë¬¸ìš©ì–´ í•´ì„¤")
        st.markdown("- íˆ¬ì ê°œë… ì„¤ëª…")
        st.markdown("- ê¸ˆìœµ ìƒí’ˆ ì •ë³´")
        st.markdown("- ì „ë¬¸ìš©ì–´ ìë™ ì£¼ì„")
        
        st.markdown("### ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ")
        st.markdown("- P/E ë¹„ìœ¨ì´ë€?")
        st.markdown("- ETF ì„¤ëª…í•´ì¤˜")
        st.markdown("- ë°°ë‹¹ìˆ˜ìµë¥ ")
        st.markdown("- ì˜µì…˜ ê±°ë˜")
    
    st.markdown("---")
    st.markdown("### ğŸ“‹ í•„ìš” ì„¤ì¹˜")
    st.code("pip install playwright beautifulsoup4")
    st.code("playwright install chromium")
    st.code("pip install datasets langchain")
    
    if PLAYWRIGHT_AVAILABLE:
        st.success("âœ… Playwright ì„¤ì¹˜ë¨")
    else:
        st.error("âŒ Playwright ì„¤ì¹˜ í•„ìš”")

import streamlit as st
import asyncio

async def async_stream(query):
    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
    async for event in graph.astream({"query": query}, debug=True):
        yield event

def run_async_stream(query):
    # Streamlit ë™ê¸° í•¨ìˆ˜ì—ì„œ ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” í—¬í¼
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    gen = async_stream(query)
    
    async def gather_events():
        events = []
        async for event in gen:
            events.append(event)
        return events

    return loop.run_until_complete(gather_events())

if query := st.chat_input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
    if not PLAYWRIGHT_AVAILABLE:
        st.error("Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì˜ ì„¤ì¹˜ ëª…ë ¹ì–´ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.")
    else:
        st.subheader(f"ğŸ” ê²€ìƒ‰: {query}")
        st.subheader("ğŸ¤– ë‹µë³€")
        with st.spinner("ë‹µë³€ ìƒì„±ì¤‘..."):
            try:
                # ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ì—ì„œ ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ë°›ì•„ì˜´ (ë™ê¸° í•¨ìˆ˜ ì•ˆì—ì„œ ì‹¤í–‰)
                events = run_async_stream(query)

                # ë°›ì€ ì´ë²¤íŠ¸ë“¤ì„ í™”ë©´ì— ì¶œë ¥
                for event in events:
                    for k, v in event.items():
                        if k == 'extract_keyword':
                            with st.container():
                                st.write("### ğŸ”‘ ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œ")
                                st.markdown(f"**ì›ë³¸ ì§ˆë¬¸:** {query}")
                                st.markdown(f"**ì¶”ì¶œëœ í‚¤ì›Œë“œ:** {v['extracted_keyword']}")
                        
                        elif k == 'query_expansion':
                            with st.container():
                                st.write("### ğŸ” í™•ì¥ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸")
                                expanded_query_md = '\n'.join([f"- {q}" for q in v['expanded_query_list']])
                                st.markdown(expanded_query_md)
                        
                        elif k == 'search':
                            with st.expander("ğŸ“° ê²€ìƒ‰ëœ ë‰´ìŠ¤ (EconoTimes)"):
                                for search_item in v['search_results']:
                                    with st.container():
                                        st.markdown(f"**ì œëª©:** {search_item['title']}")
                                        st.markdown(f"**ê²€ìƒ‰ ì¿¼ë¦¬:** {search_item['search_query']}")
                                        st.markdown(f"**URL:** [{search_item['url']}]({search_item['url']})")
                                        st.markdown(f"**ë‚´ìš©:** {search_item['content'][:500]}...")
                                        
                        elif k == 'general economic':
                            with st.expander("ğŸ“° ê²€ìƒ‰ëœ ë‰´ìŠ¤ (ì¼ë°˜ ê²½ì œ ì‹œì¥)"):
                                for search_item in v['search_results2']:
                                    with st.container():
                                        st.markdown(f"**ì œëª©:** {search_item['title']}")
                                        st.markdown(f"**ê²€ìƒ‰ ì¿¼ë¦¬:** {search_item['search_query']}")
                                        st.markdown(f"**URL:** [{search_item['url']}]({search_item['url']})")
                                        st.markdown(f"**ë‚´ìš©:** {search_item['content'][:500]}...")
                        
                        elif k == 'make_analysis_for_each_query':
                            with st.expander("ğŸ“Š ë‰´ìŠ¤ ë¶„ì„"):
                                for analysis in v['analysis_list']:
                                    st.markdown(f"**ê²€ìƒ‰ ì¿¼ë¦¬:** {analysis['search_query']}")
                                    st.markdown(f"**ê´€ë ¨ ë‰´ìŠ¤ ì¡´ì¬:** {'âœ…' if analysis['has_relevant_news'] else 'âŒ'}")
                                    if analysis['has_relevant_news']:
                                        st.markdown(f"**ë¶„ì„ ìš”ì•½:** {analysis['analysis_summary']}")
                                    st.markdown("---")
                                    
                       
                        elif k == 'generate':
                            st.markdown("## ğŸ“‹ ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸")
                            st.markdown(v['answer'])

            except Exception as e:
                st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.info("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

# í‘¸í„°
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    <p>Powered by MIRAE ASSET + Naver ClovaX </p>
    <p><small>âš ï¸ íˆ¬ì ê²°ì •ì€ ì‹ ì¤‘íˆ í•˜ì‹œê³ , ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤.</small></p>
</div>
""", unsafe_allow_html=True)