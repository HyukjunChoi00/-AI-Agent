import json
from typing import TypedDict, Annotated, List, Dict, Optional
import operator
import os
import asyncio
import re
from urllib.parse import urlparse, urljoin
import nest_asyncio
nest_asyncio.apply()
import sys
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
import streamlit as st
from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
)

# Playwright ê´€ë ¨ ì„í¬íŠ¸

try:
    from playwright.async_api import async_playwright
    from bs4 import BeautifulSoup, Comment
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    st.error("Playwright ë˜ëŠ” BeautifulSoupì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
    st.code("pip install playwright beautifulsoup4")
    st.code("playwright install chromium")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["CLOVASTUDIO_API_KEY"] = ''

from langchain_naver import ChatClovaX
  
chat = ChatClovaX(
    model="HCX-DASH-002" # ëª¨ë¸ëª… ì…ë ¥ (ê¸°ë³¸ê°’: HCX-005) 
)

### Econotimesë¡œë¶€í„° ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ í´ë˜ìŠ¤ ì •ì˜  ##################

class EconoTimesFullPipeline:
    def __init__(self, max_articles=5):
        self.max_articles = max_articles
        self.base_url = "https://www.econotimes.com"

    async def setup_browser(self):
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(headless=True)
        self.context = await self.browser.new_context()

    async def close_browser(self):
        await self.browser.close()
        await self.playwright.stop()

    def clean_title(self, title: str) -> str:
        title = re.sub(r'\s+', ' ', title.strip())
        title = re.sub(r'^\W+|\W+$', '', title)
        return title if len(title) > 10 else ""

    async def extract_articles(self, page, query: str) -> List[Dict[str, str]]:
        articles = []
        for i in range(1, self.max_articles + 1):
            try:
                xpath = f'//*[@id="archivePage"]/div/div[2]/div[{i}]/p[1]/a'
                link = await page.query_selector(f'xpath={xpath}')
                if not link:
                    continue
                title = await link.inner_text()
                url = await link.get_attribute('href')
                url = urljoin(self.base_url, url) if url and not url.startswith("http") else url
                title = self.clean_title(title)
                if title and url:
                    articles.append({"title": title, "url": url})
            except:
                continue
        return articles

    async def fetch_html(self, url: str) -> str:
        page = await self.context.new_page()
        await page.goto(url, wait_until='domcontentloaded', timeout=20000)
        await asyncio.sleep(3)
        html = await page.inner_html("body")
        await page.close()
        return html

    def clean_html(self, raw_html: str) -> str:
        soup = BeautifulSoup(raw_html, 'html.parser')
        for tag in soup.find_all(True):
            tag.attrs = {}
        for tag in soup(['script', 'style', 'header', 'footer', 'nav']):
            tag.decompose()
        for comment in soup.find_all(text=lambda t: isinstance(t, Comment)):
            comment.extract()
        return str(soup)

    def extract_text(self, html: str) -> str:
        soup = BeautifulSoup(html, 'html.parser')
        article = soup.find('article')
        if not article:
            return "ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        for tag in article(['ins', 'iframe', 'script']):
            tag.decompose()
        paragraphs = [p.get_text(strip=True) for p in article.find_all('p')]
        return '\n\n'.join(paragraphs)

    async def run_pipeline(self, query: str) -> List[Dict[str, str]]:
        await self.setup_browser()
        search_url = f"https://www.econotimes.com/search?v={query}"
        page = await self.context.new_page()
        await page.goto(search_url, wait_until='domcontentloaded', timeout=20000)
        await asyncio.sleep(2)
        articles = await self.extract_articles(page, query)
        await page.close()

        results = []
        for article in articles:
            html = await self.fetch_html(article["url"])
            cleaned_html = self.clean_html(html)
            text = self.extract_text(cleaned_html)
            results.append({
                "title": article["title"],
                "url": article["url"],
                "content": text
            })

        await self.close_browser()
        return results

####################################################################################################################
## ì¿¼ë¦¬ í™•ì¥ íŒŒíŠ¸

class ExpandedQuery(BaseModel):
    expanded_search_query_list: List[str] = Field(description="ì§ˆì˜ í™•ì¥ëœ ê²€ìƒ‰ í‚¤ì›Œë“œí˜• ì§ˆì˜ ë¦¬ìŠ¤íŠ¸. í‚¤ì›Œë“œí˜• ê²€ìƒ‰ ì—”ì§„ì„ ìœ„í•´")

# í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ Chain
def build_keyword_extraction_chain():
    prompt_template = ChatPromptTemplate.from_messages(
        [("system", "ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ í•µì‹¬ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” Assistantì´ë‹¤."),
         ("user", "ë‹¤ìŒ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ë¼. ë°˜ë“œì‹œ ì˜ë¬¸ í‚¤ì›Œë“œë¡œ ì œê³µí•˜ë¼.\n"
                  "ì˜ˆì‹œ:\n"
                  "- 'NVIDIA ì£¼ì‹ ë™í–¥ ì¡°ì‚¬ì¢€ í•´ì¤˜' â†’ 'NVIDIA'\n"
                  "- 'í…ŒìŠ¬ë¼ ì£¼ê°€ ì „ë§ì´ ì–´ë•Œ?' â†’ 'Tesla'\n"
                  "- 'ì• í”Œ ì•„ì´í° ìµœì‹  ì†Œì‹ ì•Œë ¤ì¤˜' â†’ 'Apple'\n"
                  "- 'AI ê¸°ìˆ  ë°œì „ í˜„í™©' â†’ 'AI'\n"
                  "ì§ˆë¬¸: {query}\n"
                  "í•µì‹¬ í‚¤ì›Œë“œ:")]
    )
    return prompt_template | chat
    )


# Chain ì •ì˜
def build_query_expansion_chain():
    prompt_template = ChatPromptTemplate.from_messages(
        [("system", "ë„ˆëŠ” í‚¤ì›Œë“œí˜• Query Expansion Assistantì´ë‹¤."),
         ("user", "ë‹¤ìŒ ìœ ì €ì˜ ì§ˆì˜ë¥¼ í™•ì¥í•˜ë¼. ìœ ì €ê°€ ì…ë ¥í•œ ì£¼ì‹ ì¢…ëª©ê³¼ ê´€ë ¨ëœ ì‚°ì—…ì— ëŒ€í•œ, ì£¼ìš” í‚¤ì›Œë“œë¡œ í™•ì¥í•˜ë¼. ë°˜ë“œì‹œ ì˜ë¬¸ í‚¤ì›Œë“œë§Œ ì´ìš©í•˜ë¼.ì˜ˆë¥¼ ë“¤ì–´, NVIDIAì— ëŒ€í•œ í‚¤ì›Œë“œë¥¼ ë°›ìœ¼ë©´ AI, GPU ë“±ìœ¼ë¡œ í™•ì¥í•  ìˆ˜ ìˆë‹¤.\n"
                  "ì§ˆì˜: {query}\n"
                  "í™•ì¥ ì§ˆì˜ ê°¯ìˆ˜: {n}\n"
                  "ì§ˆì˜ '{query}'ì— ëŒ€í•´ {n}ê°œì˜ í™•ì¥ëœ í‚¤ì›Œë“œí˜• ì§ˆì˜ë¥¼ ë§Œë“¤ì–´ë¼.")]
    )
    query_expansion_chain = prompt_template | chat.with_structured_output(ExpandedQuery)
    
    return query_expansion_chain

#######################################################################################################################

class NewsAnalysis(BaseModel):
    """ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼"""
    has_relevant_news: bool = Field(description="ê´€ë ¨ ë‰´ìŠ¤ê°€ ìˆëŠ”ì§€ ì—¬ë¶€")
    analysis_summary: str = Field(description="ë‰´ìŠ¤ ë¶„ì„ ìš”ì•½. bullet pointë¥¼ í™œìš©í•œ ê°œì¡°ì‹ ë‹µë³€. ë§Œì•½ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ê³µë°±ìœ¼ë¡œ ë°˜í™˜")

def build_news_analysis_chain():
    prompt_template = ChatPromptTemplate.from_messages(
        [("system", "ë„ˆëŠ” ê²½ì œ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì´ë‹¤."),
         ("user", "ë‹¤ìŒ ìœ ì €ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë‰´ìŠ¤ë‚´ì— ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¶„ì„ì„ ë§Œë“¤ì–´ë¼\n"
                  "ì§ˆë¬¸: {query}\n"
                  "ê²€ìƒ‰ëœ ë‰´ìŠ¤: {news}\n"
                  "ê²€ìƒ‰ëœ ë‰´ìŠ¤ì—ì„œ '{query}'ì— ëŒ€í•´ ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¶„ì„ì„ ë§Œë“¤ì–´ë¼\n"
                  "ì‚¬ìš©ìê°€ íŠ¹ì • ì£¼ì‹ ì¢…ëª©ì— ëŒ€í•œ ë¶„ì„ì„ ìš”ì²­í–ˆë‹¤ë©´, ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”:\n"
                  "1. ì£¼ìš” íŠ¸ë Œë“œì™€ ë™í–¥ ìš”ì•½\n"
                  "2. ì‹œì¥ ì˜í–¥ ë¶„ì„\n"
                  "3. í–¥í›„ ì „ë§ì´ë‚˜ ì£¼ëª©í•  ì \n"
                  "4. ì „ë°˜ì ì¸ ê²½ì œ ìƒí™©")]
    )
    
    news_analysis_chain = prompt_template | chat.with_structured_output(NewsAnalysis)
    
    return news_analysis_chain



# ê·¸ë˜í”„ ìƒíƒœ ì •ì˜
class State(TypedDict):
    query: str
    extracted_keyword: str
    expanded_query_list: List[str]
    analysis_list: List[str]
    answer: str
    messages: Annotated[List[BaseMessage], operator.add]
    search_results: List[dict]
    search_results2 : List[dict]


# ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
def extract_keyword(state):
    """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë…¸ë“œ"""
    keyword_extraction_chain = build_keyword_extraction_chain()
    original_query = state["query"]
    
    # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
    extracted_keyword = keyword_extraction_chain.invoke({"query": original_query})
    
    # ì¶”ì¶œëœ í‚¤ì›Œë“œ ì •ë¦¬ (ê³µë°± ì œê±°, ì²« ê¸€ì ëŒ€ë¬¸ì ë“±)
    keyword = extracted_keyword.content.strip()
    
    return {"extracted_keyword": keyword}


def query_expansion(state):
    query_expansion_chain = build_query_expansion_chain()
    extracted_keyword = state["extracted_keyword"]
    
    expanded_query_obj = query_expansion_chain.invoke({"query": extracted_keyword,
                                                       "n": 2}) 
    expanded_query_list = [extracted_keyword, *expanded_query_obj.expanded_search_query_list]

    return {"expanded_query_list": expanded_query_list}

############################################################################### ì•„ë˜ ë¹„ë™ê¸° í•¨ìˆ˜ ì²˜ë¦¬ê°€ ì œì¼ ë¬¸ì œì„.

async def search(state):
    expanded_query_list = state["expanded_query_list"]
    pipeline = EconoTimesFullPipeline(max_articles=3)
    all_results = []
    await pipeline.setup_browser()
    try:
        for query in expanded_query_list:
            try:
                results = await pipeline.run_pipeline(query)
                if not results:
                    # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë‹¤ìŒ ì¿¼ë¦¬ë¡œ
                    continue
                
                # ê° ê²°ê³¼ì— ê²€ìƒ‰ ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
                all_results.extend([{
                    "title": article["title"],
                    "url": article["url"],
                    "content": article["content"],
                    "search_query": query
                } for article in results])
                
            except Exception as e:
                # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê¹…í•˜ê±°ë‚˜ ë¬´ì‹œí•˜ê³  ë‹¤ìŒ ì¿¼ë¦¬ ê³„ì† ì§„í–‰ ê°€ëŠ¥
                # í•„ìš”í•˜ë©´ ì•„ë˜ ì£¼ì„ ì œê±° í›„ ë¡œê¹… ì¶”ê°€
                # print(f"Error processing query '{query}': {e}")
                continue
    finally:
        await pipeline.close_browser()

    
    return {"search_results": all_results}


#############################################################################

async def general_economics(state):
    pipeline = EconoTimesFullPipeline(max_articles=3)
    expanded_query_list = ['tariff', 'interest rate']
    all_results = []
    await pipeline.setup_browser()
    try:
        for query in expanded_query_list:
            try:
                results = await pipeline.run_pipeline(query)
                if not results:
                    # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë‹¤ìŒ ì¿¼ë¦¬ë¡œ
                    continue
                
                # ê° ê²°ê³¼ì— ê²€ìƒ‰ ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
                all_results.extend([{
                    "title": article["title"],
                    "url": article["url"],
                    "content": article["content"],
                    "search_query": query
                } for article in results])
                
            except Exception as e:
                # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê¹…í•˜ê±°ë‚˜ ë¬´ì‹œí•˜ê³  ë‹¤ìŒ ì¿¼ë¦¬ ê³„ì† ì§„í–‰ ê°€ëŠ¥
                # í•„ìš”í•˜ë©´ ì•„ë˜ ì£¼ì„ ì œê±° í›„ ë¡œê¹… ì¶”ê°€
                # print(f"Error processing query '{query}': {e}")
                continue
    finally:
        await pipeline.close_browser()

    
    return {"search_results2": all_results}

#############################################################################

def make_analysis_for_each_query(state):
    news_analysis_chain = build_news_analysis_chain()

    search_results = state["search_results"]
    search_results += state["search_results2"]
    
    analysis_obj_list = news_analysis_chain.batch([{"query": doc['search_query'],
                                                   "news": doc['content']}
                                                   for doc in search_results])

    analysis_list = []
    for analysis_obj, search_item in zip(analysis_obj_list, search_results):
        analysis_list.append({**analysis_obj.dict(),
                             "search_query": search_item['search_query'],
                             "title": search_item['title'],
                             "url": search_item['url'],
                             "content": search_item['content']
                             })
    return {"analysis_list": analysis_list}

#############################################################################

def generate_response(state):
    context = "\n\n".join([analysis['analysis_summary']
                           for analysis in state["analysis_list"]
                           if analysis['has_relevant_news']])
    
    curr_human_turn = HumanMessage(content=f"ì§ˆë¬¸: {state['query']}\n"
                            f"```\n{context}```"
                             "\n---\n"
                             "ì‘ë‹µì€ markdownì„ ì´ìš©í•´ ë¦¬í¬íŠ¸ ìŠ¤íƒ€ì¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ë¼. "
                             "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì˜ ì˜ë„ì— ë§ëŠ” ì •ë‹µ ë¶€ë¶„ì„ ê°•ì¡°í•´ë¼.")
    messages = state["messages"] + [curr_human_turn]
    response = llm.invoke(messages)

    return {"messages": [*messages, response],
            "answer": response.content}

# Streamlit ì•± ì´ˆê¸°í™”
if 'graph' not in st.session_state:
    llm = chat

    # ê·¸ë˜í”„ êµ¬ì„±
    workflow = StateGraph(State)
    workflow.add_node("extract_keyword", extract_keyword)
    workflow.add_node("query_expansion", query_expansion)
    workflow.add_node("search", search)
    workflow.add_node("general economic", general_economics)
    workflow.add_node("make_analysis_for_each_query", make_analysis_for_each_query)
    workflow.add_node("generate", generate_response)
    workflow.add_edge("extract_keyword", "query_expansion")
    workflow.add_edge("query_expansion", "search")
    workflow.add_edge("search", "general economic")
    workflow.add_edge("general economic", "make_analysis_for_each_query")
    workflow.add_edge("make_analysis_for_each_query", "generate")

    workflow.set_entry_point("extract_keyword")
    workflow.set_finish_point("generate")

    graph = workflow.compile()

    st.session_state.graph = graph
    st.session_state.llm = llm

graph = st.session_state.graph 
llm = st.session_state.llm 

###############################################################################
# Streamlit View
st.title("ğŸ“° ê¸ˆìœµ íˆ¬ì ì§€ì› ì±—ë´‡ (EconoTimes)")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ğŸ¤– ì±—ë´‡ ì„ íƒ")
    
    chatbot_mode = st.selectbox(
        "ì±—ë´‡ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        ["íˆ¬ì ì˜ì‚¬ê²°ì • ì§€ì› ì±—ë´‡", "ê¸ˆìœµ ì‚¬ì „ ì±—ë´‡"],
        index=0
    )
    
    st.markdown("---")
    
    if chatbot_mode == "íˆ¬ì ì˜ì‚¬ê²°ì • ì§€ì› ì±—ë´‡":
        st.markdown("### ğŸ“ˆ íˆ¬ì ì˜ì‚¬ê²°ì • ì§€ì›")
        st.markdown("**ê¸°ëŠ¥:**")
        st.markdown("- ì‹¤ì‹œê°„ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„")
        st.markdown("- ì¢…ëª©ë³„ ì‹œì¥ ë™í–¥ ë¶„ì„")
        st.markdown("- íˆ¬ì ê´€ë ¨ ë¦¬í¬íŠ¸ ìƒì„±")
        
        st.markdown("### ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ")
        st.markdown("- NVIDIA ì£¼ì‹ ë™í–¥")
        st.markdown("- Tesla ì „ë§")
        st.markdown("- Apple ì‹¤ì  ë¶„ì„")
        st.markdown("- AI ê´€ë ¨ íˆ¬ì")
        
    else:
        st.markdown("### ğŸ“š ê¸ˆìœµ ì‚¬ì „")
        st.markdown("**ê¸°ëŠ¥:**")
        st.markdown("- ê¸ˆìœµ ìš©ì–´ ì„¤ëª…")
        st.markdown("- íˆ¬ì ê°œë… í•´ì„¤")
        st.markdown("- ê¸ˆìœµ ìƒí’ˆ ì •ë³´")
        
        st.markdown("### ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ")
        st.markdown("- P/E ë¹„ìœ¨ì´ë€?")
        st.markdown("- ETF ì„¤ëª…í•´ì¤˜")
        st.markdown("- ë°°ë‹¹ìˆ˜ìµë¥ ")
        st.markdown("- ì˜µì…˜ ê±°ë˜")
    
    st.markdown("---")
    st.markdown("### ğŸ“‹ í•„ìš” ì„¤ì¹˜")
    st.code("pip install playwright beautifulsoup4")
    st.code("playwright install chromium")
    
    if PLAYWRIGHT_AVAILABLE:
        st.success("âœ… Playwright ì„¤ì¹˜ë¨")
    else:
        st.error("âŒ Playwright ì„¤ì¹˜ í•„ìš”")

import streamlit as st
import asyncio

# graphëŠ” ì´ë¯¸ ì¤€ë¹„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •

async def async_stream(query):
    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
    async for event in graph.astream({"query": query}, debug=True):
        yield event

def run_async_stream(query):
    # Streamlit ë™ê¸° í•¨ìˆ˜ì—ì„œ ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” í—¬í¼
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    gen = async_stream(query)
    
    async def gather_events():
        events = []
        async for event in gen:
            events.append(event)
        return events

    return loop.run_until_complete(gather_events())

if query := st.chat_input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
    if not PLAYWRIGHT_AVAILABLE:
        st.error("Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì˜ ì„¤ì¹˜ ëª…ë ¹ì–´ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.")
    else:
        st.subheader(f"ğŸ” ê²€ìƒ‰: {query}")
        st.subheader("ğŸ¤– ë‹µë³€")
        with st.spinner("ë‹µë³€ ìƒì„±ì¤‘..."):
            try:
                # ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ì—ì„œ ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ë°›ì•„ì˜´ (ë™ê¸° í•¨ìˆ˜ ì•ˆì—ì„œ ì‹¤í–‰)
                events = run_async_stream(query)

                # ë°›ì€ ì´ë²¤íŠ¸ë“¤ì„ í™”ë©´ì— ì¶œë ¥
                for event in events:
                    for k, v in event.items():
                        if k == 'extract_keyword':
                            with st.container():
                                st.write("### ğŸ”‘ ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œ")
                                st.markdown(f"**ì›ë³¸ ì§ˆë¬¸:** {query}")
                                st.markdown(f"**ì¶”ì¶œëœ í‚¤ì›Œë“œ:** {v['extracted_keyword']}")
                        
                        elif k == 'query_expansion':
                            with st.container():
                                st.write("### ğŸ” í™•ì¥ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸")
                                expanded_query_md = '\n'.join([f"- {q}" for q in v['expanded_query_list']])
                                st.markdown(expanded_query_md)
                        
                        elif k == 'search':
                            with st.expander("ğŸ“° ê²€ìƒ‰ëœ ë‰´ìŠ¤ (EconoTimes)"):
                                for search_item in v['search_results']:
                                    with st.container():
                                        st.markdown(f"**ì œëª©:** {search_item['title']}")
                                        st.markdown(f"**ê²€ìƒ‰ ì¿¼ë¦¬:** {search_item['search_query']}")
                                        st.markdown(f"**URL:** [{search_item['url']}]({search_item['url']})")
                                        st.markdown(f"**ë‚´ìš©:** {search_item['content'][:500]}...")
                                        
                        elif k == 'general economic':
                            with st.expander("ğŸ“° ê²€ìƒ‰ëœ ë‰´ìŠ¤ (ì¼ë°˜ ê²½ì œ ì‹œì¥)"):
                                for search_item in v['search_results2']:
                                    with st.container():
                                        st.markdown(f"**ì œëª©:** {search_item['title']}")
                                        st.markdown(f"**ê²€ìƒ‰ ì¿¼ë¦¬:** {search_item['search_query']}")
                                        st.markdown(f"**URL:** [{search_item['url']}]({search_item['url']})")
                                        st.markdown(f"**ë‚´ìš©:** {search_item['content'][:500]}...")
                        
                        elif k == 'make_analysis_for_each_query':
                            with st.expander("ğŸ“Š ë‰´ìŠ¤ ë¶„ì„"):
                                for analysis in v['analysis_list']:
                                    st.markdown(f"**ê²€ìƒ‰ ì¿¼ë¦¬:** {analysis['search_query']}")
                                    st.markdown(f"**ê´€ë ¨ ë‰´ìŠ¤ ì¡´ì¬:** {'âœ…' if analysis['has_relevant_news'] else 'âŒ'}")
                                    if analysis['has_relevant_news']:
                                        st.markdown(f"**ë¶„ì„ ìš”ì•½:** {analysis['analysis_summary']}")
                                    st.markdown("---")
                                    
                       
                        elif k == 'generate':
                            st.markdown("## ğŸ“‹ ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸")
                            st.markdown(v['answer'])

            except Exception as e:
                st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.info("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")


# í‘¸í„°
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    <p>Powered by Streamlit + LangGraph + Google Gemini + EconoTimes</p>
    <p><small>âš ï¸ íˆ¬ì ê²°ì •ì€ ì‹ ì¤‘íˆ í•˜ì‹œê³ , ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤.</small></p>
</div>
""", unsafe_allow_html=True)
